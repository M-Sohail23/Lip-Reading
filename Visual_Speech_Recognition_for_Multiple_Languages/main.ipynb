{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T10:38:18.623496Z",
     "start_time": "2024-05-26T10:38:14.420616Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from pipelines.model import AVSR\n",
    "from pipelines.data.data_module import AVSRDataLoader\n",
    "from pipelines.detectors.mediapipe.detector import LandmarksDetector\n",
    "\n",
    "class InferencePipeline(torch.nn.Module):\n",
    "    def __init__(self, modality, model_path, model_conf, detector=\"mediapipe\", face_track=False, device=\"metal\"):\n",
    "        super(InferencePipeline, self).__init__()\n",
    "        self.device = device\n",
    "        # modality configuration\n",
    "        self.modality = modality\n",
    "        self.dataloader = AVSRDataLoader(modality, detector=detector)\n",
    "        self.model = AVSR(modality, model_path, model_conf\n",
    "        , rnnlm=None, rnnlm_conf=None, penalty=0.0, ctc_weight=0.1, lm_weight=0.0, beam_size=40, device=device)\n",
    "        if face_track and self.modality in [\"video\", \"audiovisual\"]:\n",
    "            self.landmarks_detector = LandmarksDetector()\n",
    "        else:\n",
    "            self.landmarks_detector = None\n",
    "\n",
    "\n",
    "    def process_landmarks(self, data_filename, landmarks_filename):\n",
    "        if self.modality == \"audio\":\n",
    "            return None\n",
    "        if self.modality in [\"video\", \"audiovisual\"]:\n",
    "            landmarks = self.landmarks_detector(data_filename)\n",
    "            return landmarks\n",
    "\n",
    "\n",
    "    def forward(self, data_filename, landmarks_filename=None):\n",
    "        assert os.path.isfile(data_filename), f\"data_filename: {data_filename} does not exist.\"\n",
    "        landmarks = self.process_landmarks(data_filename, landmarks_filename)\n",
    "        data = self.dataloader.load_data(data_filename, landmarks)\n",
    "        transcript = self.model.infer(data)\n",
    "        return transcript\n",
    "\n",
    "    def extract_features(self, data_filename, landmarks_filename=None, extract_resnet_feats=False):\n",
    "        assert os.path.isfile(data_filename), f\"data_filename: {data_filename} does not exist.\"\n",
    "        landmarks = self.process_landmarks(data_filename, landmarks_filename)\n",
    "        data = self.dataloader.load_data(data_filename, landmarks)\n",
    "        with torch.no_grad():\n",
    "            if isinstance(data, tuple):\n",
    "                enc_feats = self.model.model.encode(data[0].to(self.device), data[1].to(self.device), extract_resnet_feats)\n",
    "            else:\n",
    "                enc_feats = self.model.model.encode(data.to(self.device), extract_resnet_feats)\n",
    "        return enc_feats"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T10:38:21.578798Z",
     "start_time": "2024-05-26T10:38:18.625728Z"
    }
   },
   "source": [
    "modality = \"video\"\n",
    "model_conf = \"LRS3_V_WER19.1/model.json\"\n",
    "model_path = \"LRS3_V_WER19.1/model.pth\"\n",
    "pipeline = InferencePipeline(modality, model_path, model_conf, face_track=True, device='cpu')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1716719901.566971  686705 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "I0000 00:00:1716719901.572232  686705 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T10:40:07.049278Z",
     "start_time": "2024-05-26T10:39:08.662811Z"
    }
   },
   "source": "transcript = pipeline(\"/Users/mohammedthansheer/Desktop/Test data/Audioless/Azzhan.mov\")",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONTH SO I ENCOURAGE YOU TO EMBRACE YOUR CURIOSITY TO APPROACH EACH DAY AND WONDER AND OPENNESS WHO KNOWS WHAT AMAZING DISCOVERIES HAVE ENTERED\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": "print(transcript) #Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia,"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T10:46:21.894489Z",
     "start_time": "2024-05-26T10:45:25.248279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from pipelines.model import AVSR\n",
    "from pipelines.data.data_module import AVSRDataLoader\n",
    "from pipelines.detectors.mediapipe.detector import LandmarksDetector\n",
    "\n",
    "# Initialize the inference pipeline\n",
    "modality = \"video\"\n",
    "model_conf = \"LRS3_V_WER19.1/model.json\"\n",
    "model_path = \"LRS3_V_WER19.1/model.pth\"\n",
    "pipeline = InferencePipeline(modality, model_path, model_conf, face_track=True, device='cpu')\n",
    "\n",
    "# Perform inference on sample input data\n",
    "transcript = pipeline(\"/Users/mohammedthansheer/Desktop/Test data/Audioless/Azzhan.mov\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1716720328.078724  686705 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "I0000 00:00:1716720328.089694  686705 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T10:50:29.506404Z",
     "start_time": "2024-05-26T10:49:37.488911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform inference on sample input data\n",
    "transcript = pipeline(\"/Users/mohammedthansheer/Desktop/Test data/Audioless/Azzhan.mov\")\n",
    "\n",
    "# Calculate the time taken for inference\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "# Print the time taken\n",
    "print(\"Time taken for inference:\", processing_time, \"seconds\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for inference: 51.98213195800781 seconds\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T10:50:29.516996Z",
     "start_time": "2024-05-26T10:50:29.511842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the generated transcript\n",
    "print(transcript)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONTH SO I ENCOURAGE YOU TO EMBRACE YOUR CURIOSITY TO APPROACH EACH DAY AND WONDER AND OPENNESS WHO KNOWS WHAT AMAZING DISCOVERIES HAVE ENTERED\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
